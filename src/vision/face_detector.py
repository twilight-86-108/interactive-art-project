import cv2
import mediapipe as mp
import numpy as np
from typing import Dict, List, Optional, Tuple
import time

class FaceDetector:
    """MediaPipe Face Mesh çµ±åˆæ¤œå‡ºå™¨ï¼ˆDay 3ç‰ˆï¼‰"""
    
    def __init__(self, config: dict):
        self.config = config
        
        # MediaPipe Face Mesh åˆæœŸåŒ–
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # Face Mesh è¨­å®š
        face_config = config.get('ai_processing', {}).get('vision', {}).get('face_detection', {})
        
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=face_config.get('max_num_faces', 1),
            refine_landmarks=face_config.get('refine_landmarks', True),
            min_detection_confidence=face_config.get('min_detection_confidence', 0.7),
            min_tracking_confidence=face_config.get('min_tracking_confidence', 0.5)
        )
        
        # é‡è¦ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
        self.FACE_LANDMARKS = {
            'silhouette': [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                          397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,
                          172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109],
            'nose': [1, 2, 5, 4, 6, 19, 20, 94, 125, 141, 235, 236, 3, 51, 48, 115, 131, 134, 102, 49, 220, 305, 292, 308, 415, 310, 311, 312, 13, 82, 78, 14, 15, 16, 17, 18],
            'mouth': [61, 84, 17, 314, 405, 320, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95],
            'left_eye': [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246],
            'right_eye': [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398],
            'left_eyebrow': [46, 53, 52, 51, 48, 115, 131, 134, 102, 49, 220, 305, 292, 308, 415, 310, 311, 312],
            'right_eyebrow': [276, 283, 282, 295, 285, 336, 296, 334, 293, 300, 276, 283, 282, 295, 285]
        }
        
        # å‡¦ç†çµ±è¨ˆ
        self.detection_times = []
        self.detection_count = 0
        self.last_detection_result = None
        
        print("âœ… Face Detector åˆæœŸåŒ–å®Œäº†")
    
    def detect_face(self, frame: np.ndarray) -> Dict:
        """é¡”æ¤œå‡ºãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        
        try:
            # RGBå¤‰æ›ï¼ˆMediaPipeç”¨ï¼‰
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipeå‡¦ç†
            results = self.face_mesh.process(rgb_frame)
            
            # çµæœè§£æ
            detection_result = self._analyze_detection_results(results, frame.shape)
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            processing_time = time.time() - start_time
            self.detection_times.append(processing_time)
            if len(self.detection_times) > 100:
                self.detection_times.pop(0)
            
            self.detection_count += 1
            self.last_detection_result = detection_result
            
            return detection_result
            
        except Exception as e:
            print(f"âš ï¸  é¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_empty_result()
    
    def _analyze_detection_results(self, results, frame_shape) -> Dict:
        """æ¤œå‡ºçµæœè§£æ"""
        height, width = frame_shape[:2]
        
        result = {
            'face_detected': False,
            'face_count': 0,
            'landmarks': None,
            'face_center': None,
            'face_size': 0,
            'face_angle': {'yaw': 0, 'pitch': 0, 'roll': 0},
            'face_distance': 0,
            'confidence': 0.0,
            'processing_time': self.detection_times[-1] if self.detection_times else 0
        }
        
        if results.multi_face_landmarks:
            result['face_detected'] = True
            result['face_count'] = len(results.multi_face_landmarks)
            
            # æœ€åˆã®é¡”ã®ã¿å‡¦ç†ï¼ˆmax_num_faces=1ã®å ´åˆï¼‰
            face_landmarks = results.multi_face_landmarks[0]
            result['landmarks'] = face_landmarks
            
            # é¡”ä¸­å¿ƒè¨ˆç®—ï¼ˆé¼»å…ˆãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ä½¿ç”¨ï¼‰
            nose_tip = face_landmarks.landmark[1]  # é¼»å…ˆ
            result['face_center'] = (nose_tip.x, nose_tip.y, nose_tip.z)
            
            # é¡”ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆé¡”ã®å¹…ï¼‰
            left_face = face_landmarks.landmark[234]  # å·¦é¡”ç«¯
            right_face = face_landmarks.landmark[454]  # å³é¡”ç«¯
            face_width = abs(right_face.x - left_face.x)
            result['face_size'] = face_width
            
            # é¡”ã®å‘ãæ¨å®š
            result['face_angle'] = self._estimate_face_pose(face_landmarks)
            
            # è·é›¢æ¨å®šï¼ˆé¡”ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
            result['face_distance'] = self._estimate_distance(face_width)
            
            # ä¿¡é ¼åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼šãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®visibilityã‹ã‚‰ï¼‰
            confidences = [lm.visibility for lm in face_landmarks.landmark if hasattr(lm, 'visibility')]
            result['confidence'] = sum(confidences) / len(confidences) if confidences else 0.8
        
        return result
    
    def _estimate_face_pose(self, landmarks) -> Dict[str, float]:
        """é¡”ã®å‘ãæ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ä¸»è¦ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å–å¾—
        nose_tip = landmarks.landmark[1]
        chin = landmarks.landmark[152]
        left_eye = landmarks.landmark[33]
        right_eye = landmarks.landmark[263]
        left_mouth = landmarks.landmark[61]
        right_mouth = landmarks.landmark[291]
        
        # ãƒ¨ãƒ¼è§’ï¼ˆå·¦å³å›è»¢ï¼‰- ç›®ã®ä½ç½®ã‹ã‚‰æ¨å®š
        eye_center_x = (left_eye.x + right_eye.x) / 2
        nose_offset = nose_tip.x - eye_center_x
        yaw = np.degrees(np.arctan(nose_offset * 2))  # ç°¡æ˜“è¨ˆç®—
        
        # ãƒ”ãƒƒãƒè§’ï¼ˆä¸Šä¸‹å›è»¢ï¼‰- é¼»ã¨é¡ã®ä½ç½®ã‹ã‚‰æ¨å®š
        nose_chin_distance = abs(nose_tip.y - chin.y)
        pitch = (0.15 - nose_chin_distance) * 300  # ç°¡æ˜“è¨ˆç®—
        
        # ãƒ­ãƒ¼ãƒ«è§’ï¼ˆå‚¾ãï¼‰- ç›®ã®ãƒ©ã‚¤ãƒ³å‚¾ãã‹ã‚‰æ¨å®š
        eye_slope = (right_eye.y - left_eye.y) / (right_eye.x - left_eye.x) if right_eye.x != left_eye.x else 0
        roll = np.degrees(np.arctan(eye_slope))
        
        return {
            'yaw': max(-45, min(45, yaw)),      # -45ã€œ45åº¦ã«åˆ¶é™
            'pitch': max(-30, min(30, pitch)),  # -30ã€œ30åº¦ã«åˆ¶é™
            'roll': max(-30, min(30, roll))     # -30ã€œ30åº¦ã«åˆ¶é™
        }
    
    def _estimate_distance(self, face_width: float) -> float:
        """è·é›¢æ¨å®šï¼ˆé¡”å¹…ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # å®Ÿéš›ã®å¹³å‡é¡”å¹…: ç´„14cm
        # ã‚«ãƒ¡ãƒ©ã‹ã‚‰1mã®è·é›¢ã§ã®ç”»é¢ä¸Šã®é¡”å¹…ã‚’åŸºæº–ã¨ã—ãŸç°¡æ˜“è¨ˆç®—
        if face_width > 0:
            # åŸºæº–å€¤ã¯å®Ÿéš›ã®ã‚«ãƒ¡ãƒ©ãƒ»ãƒ¬ãƒ³ã‚ºç‰¹æ€§ã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦
            reference_width = 0.15  # 1mã§ã®åŸºæº–å¹…
            estimated_distance = reference_width / face_width
            return max(0.3, min(3.0, estimated_distance))  # 0.3mã€œ3mã«åˆ¶é™
        return 1.0
    
    def _get_empty_result(self) -> Dict:
        """ç©ºã®çµæœ"""
        return {
            'face_detected': False,
            'face_count': 0,
            'landmarks': None,
            'face_center': None,
            'face_size': 0,
            'face_angle': {'yaw': 0, 'pitch': 0, 'roll': 0},
            'face_distance': 0,
            'confidence': 0.0,
            'processing_time': 0
        }
    
    def draw_landmarks(self, frame: np.ndarray, detection_result: Dict, 
                      draw_all: bool = False) -> np.ndarray:
        """ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»"""
        if not detection_result['face_detected']:
            return frame
        
        landmarks = detection_result['landmarks']
        if landmarks is None:
            return frame
        
        try:
            if draw_all:
                # å…¨ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»
                self.mp_drawing.draw_landmarks(
                    frame,
                    landmarks,
                    self.mp_face_mesh.FACEMESH_CONTOURS,
                    landmark_drawing_spec=None,
                    connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_contours_style()
                )
            else:
                # ä¸»è¦ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®ã¿æç”»
                self._draw_key_landmarks(frame, landmarks)
            
            # é¡”æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæç”»
            self._draw_face_info(frame, detection_result)
            
        except Exception as e:
            print(f"âš ï¸  ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»ã‚¨ãƒ©ãƒ¼: {e}")
        
        return frame
    
    def _draw_key_landmarks(self, frame: np.ndarray, landmarks):
        """ä¸»è¦ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®ã¿æç”»"""
        height, width = frame.shape[:2]
        
        # ä¸»è¦éƒ¨ä½ã®è‰²å®šç¾©
        colors = {
            'nose': (0, 255, 0),      # ç·‘
            'mouth': (0, 0, 255),     # èµ¤
            'left_eye': (255, 0, 0),  # é’
            'right_eye': (255, 0, 0), # é’
            'left_eyebrow': (255, 255, 0),  # ã‚·ã‚¢ãƒ³
            'right_eyebrow': (255, 255, 0)  # ã‚·ã‚¢ãƒ³
        }
        
        for part_name, indices in self.FACE_LANDMARKS.items():
            if part_name in colors:
                color = colors[part_name]
                for idx in indices:
                    if idx < len(landmarks.landmark):
                        lm = landmarks.landmark[idx]
                        x = int(lm.x * width)
                        y = int(lm.y * height)
                        cv2.circle(frame, (x, y), 2, color, -1)
    
    def _draw_face_info(self, frame: np.ndarray, detection_result: Dict):
        """é¡”æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæç”»"""
        y_offset = 30
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        color = (255, 255, 255)
        thickness = 2
        
        # è·é›¢æƒ…å ±
        distance = detection_result['face_distance']
        cv2.putText(frame, f"Distance: {distance:.2f}m", (10, y_offset), 
                   font, font_scale, color, thickness)
        y_offset += 25
        
        # è§’åº¦æƒ…å ±
        angles = detection_result['face_angle']
        cv2.putText(frame, f"Yaw: {angles['yaw']:.1f}deg", (10, y_offset), 
                   font, font_scale, color, thickness)
        y_offset += 25
        
        cv2.putText(frame, f"Pitch: {angles['pitch']:.1f}deg", (10, y_offset), 
                   font, font_scale, color, thickness)
        y_offset += 25
        
        # ä¿¡é ¼åº¦
        confidence = detection_result['confidence']
        cv2.putText(frame, f"Confidence: {confidence:.2f}", (10, y_offset), 
                   font, font_scale, color, thickness)
    
    def get_performance_stats(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
        if not self.detection_times:
            return {}
        
        return {
            'avg_detection_time': sum(self.detection_times) / len(self.detection_times),
            'max_detection_time': max(self.detection_times),
            'min_detection_time': min(self.detection_times),
            'detection_count': self.detection_count,
            'fps': 1.0 / (sum(self.detection_times) / len(self.detection_times)) if self.detection_times else 0
        }
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        if self.face_mesh:
            self.face_mesh.close()
        print("Face Detector ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å®Œäº†")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
if __name__ == "__main__":
    print("ğŸ” Face Detector ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    config = {
        'ai_processing': {
            'vision': {
                'face_detection': {
                    'max_num_faces': 1,
                    'refine_landmarks': True,
                    'min_detection_confidence': 0.7,
                    'min_tracking_confidence': 0.5
                }
            }
        }
    }
    
    detector = FaceDetector(config)
    
    # ã‚«ãƒ¡ãƒ©ãƒ†ã‚¹ãƒˆ
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“")
        exit()
    
    print("ğŸ“¹ é¡”æ¤œå‡ºãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆESCã§çµ‚äº†ï¼‰...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # é¡”æ¤œå‡º
        result = detector.detect_face(frame)
        
        # çµæœæç”»
        frame_with_landmarks = detector.draw_landmarks(frame, result, draw_all=False)
        
        # çµæœè¡¨ç¤º
        cv2.imshow('Face Detection Test', frame_with_landmarks)
        
        if cv2.waitKey(1) & 0xFF == 27:  # ESCã‚­ãƒ¼
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = detector.get_performance_stats()
    print(f"ğŸ“Š æ€§èƒ½çµ±è¨ˆ: {stats}")
    
    detector.cleanup()
    print("âœ… Face Detector ãƒ†ã‚¹ãƒˆå®Œäº†")
