import cv2
import mediapipe as mp
import numpy as np
from typing import Dict, List, Optional, Tuple
import time

class HandDetector:
    """MediaPipe Hands çµ±åˆæ¤œå‡ºå™¨ï¼ˆDay 3ç‰ˆï¼‰"""
    
    def __init__(self, config: dict):
        self.config = config
        
        # MediaPipe Hands åˆæœŸåŒ–
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # Hands è¨­å®š
        hand_config = config.get('ai_processing', {}).get('vision', {}).get('hand_detection', {})
        
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=hand_config.get('max_num_hands', 2),
            min_detection_confidence=hand_config.get('min_detection_confidence', 0.7),
            min_tracking_confidence=hand_config.get('min_tracking_confidence', 0.5),
            model_complexity=hand_config.get('model_complexity', 1)
        )
        
        # æ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
        self.HAND_LANDMARKS = {
            'wrist': 0,
            'thumb': [1, 2, 3, 4],
            'index': [5, 6, 7, 8],
            'middle': [9, 10, 11, 12],
            'ring': [13, 14, 15, 16],
            'pinky': [17, 18, 19, 20]
        }
        
        # å‡¦ç†çµ±è¨ˆ
        self.detection_times = []
        self.detection_count = 0
        self.last_detection_result = None
        
        print("âœ… Hand Detector åˆæœŸåŒ–å®Œäº†")
    
    def detect_hands(self, frame: np.ndarray) -> Dict:
        """æ‰‹æ¤œå‡ºãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        
        try:
            # RGBå¤‰æ›ï¼ˆMediaPipeç”¨ï¼‰
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipeå‡¦ç†
            results = self.hands.process(rgb_frame)
            
            # çµæœè§£æ
            detection_result = self._analyze_detection_results(results, frame.shape)
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            processing_time = time.time() - start_time
            self.detection_times.append(processing_time)
            if len(self.detection_times) > 100:
                self.detection_times.pop(0)
            
            self.detection_count += 1
            self.last_detection_result = detection_result
            
            return detection_result
            
        except Exception as e:
            print(f"âš ï¸  æ‰‹æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_empty_result()
    
    def _analyze_detection_results(self, results, frame_shape) -> Dict:
        """æ¤œå‡ºçµæœè§£æ"""
        height, width = frame_shape[:2]
        
        result = {
            'hands_detected': False,
            'hand_count': 0,
            'hands': [],
            'processing_time': self.detection_times[-1] if self.detection_times else 0
        }
        
        if results.multi_hand_landmarks and results.multi_handedness:
            result['hands_detected'] = True
            result['hand_count'] = len(results.multi_hand_landmarks)
            
            # å„æ‰‹ã®æƒ…å ±ã‚’è§£æ
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                hand_info = self._analyze_single_hand(hand_landmarks, handedness, (width, height))
                result['hands'].append(hand_info)
        
        return result
    
    def _analyze_single_hand(self, landmarks, handedness, frame_size) -> Dict:
        """å˜ä¸€ã®æ‰‹ã®è§£æ"""
        width, height = frame_size
        
        # æ‰‹ã®åŸºæœ¬æƒ…å ±
        hand_label = handedness.classification[0].label  # "Left" or "Right"
        hand_score = handedness.classification[0].score
        
        # æ‰‹é¦–ä½ç½®
        wrist = landmarks.landmark[0]
        wrist_pos = (wrist.x, wrist.y)
        
        # æŒ‡ã®çŠ¶æ…‹è§£æ
        finger_states = self._analyze_finger_states(landmarks)
        
        # æ‰‹ã®å‘ããƒ»è§’åº¦
        hand_angle = self._calculate_hand_angle(landmarks)
        
        # æ‰‹ã®ã‚µã‚¤ã‚º
        hand_size = self._calculate_hand_size(landmarks)
        
        # åŸºæœ¬ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜
        gesture = self._recognize_basic_gesture(finger_states, landmarks)
        
        return {
            'label': hand_label,
            'confidence': hand_score,
            'landmarks': landmarks,
            'wrist_position': wrist_pos,
            'finger_states': finger_states,
            'hand_angle': hand_angle,
            'hand_size': hand_size,
            'gesture': gesture
        }
    
    def _analyze_finger_states(self, landmarks) -> Dict[str, bool]:
        """æŒ‡ã®çŠ¶æ…‹è§£æï¼ˆä¼¸å±•/å±ˆæ›²ï¼‰"""
        finger_states = {}
        
        # è¦ªæŒ‡ï¼ˆæ°´å¹³æ–¹å‘ã®åˆ¤å®šï¼‰
        thumb_tip = landmarks.landmark[4]
        thumb_ip = landmarks.landmark[3]
        thumb_mcp = landmarks.landmark[2]
        thumb_extended = abs(thumb_tip.x - thumb_mcp.x) > abs(thumb_ip.x - thumb_mcp.x)
        finger_states['thumb'] = thumb_extended
        
        # ä»–ã®æŒ‡ï¼ˆå‚ç›´æ–¹å‘ã®åˆ¤å®šï¼‰
        fingers = ['index', 'middle', 'ring', 'pinky']
        finger_tips = [8, 12, 16, 20]
        finger_pips = [6, 10, 14, 18]
        
        for finger, tip_idx, pip_idx in zip(fingers, finger_tips, finger_pips):
            tip = landmarks.landmark[tip_idx]
            pip = landmarks.landmark[pip_idx]
            # æŒ‡å…ˆãŒPIPé–¢ç¯€ã‚ˆã‚Šä¸Šã«ã‚ã‚‹å ´åˆã¯ä¼¸å±•
            extended = tip.y < pip.y
            finger_states[finger] = extended
        
        return finger_states
    
    def _calculate_hand_angle(self, landmarks) -> float:
        """æ‰‹ã®è§’åº¦è¨ˆç®—"""
        # æ‰‹é¦–ã‹ã‚‰ä¸­æŒ‡MCPã¸ã®ãƒ™ã‚¯ãƒˆãƒ«
        wrist = landmarks.landmark[0]
        middle_mcp = landmarks.landmark[9]
        
        # è§’åº¦è¨ˆç®—
        dx = middle_mcp.x - wrist.x
        dy = middle_mcp.y - wrist.y
        angle = np.degrees(np.arctan2(dy, dx))
        
        return angle
    
    def _calculate_hand_size(self, landmarks) -> float:
        """æ‰‹ã®ã‚µã‚¤ã‚ºè¨ˆç®—"""
        # æ‰‹é¦–ã‹ã‚‰ä¸­æŒ‡å…ˆç«¯ã¾ã§ã®è·é›¢
        wrist = landmarks.landmark[0]
        middle_tip = landmarks.landmark[12]
        
        distance = np.sqrt((middle_tip.x - wrist.x)**2 + (middle_tip.y - wrist.y)**2)
        return distance
    
    def _recognize_basic_gesture(self, finger_states: Dict[str, bool], landmarks) -> str:
        """åŸºæœ¬ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜"""
        # æŒ‡ã®çŠ¶æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰åˆ¤å®š
        extended_fingers = [finger for finger, extended in finger_states.items() if extended]
        extended_count = len(extended_fingers)
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
        if extended_count == 0:
            return "fist"  # æ¡ã‚Šã“ã¶ã—
        elif extended_count == 1:
            if finger_states['index']:
                return "point"  # æŒ‡å·®ã—
            elif finger_states['thumb']:
                return "thumbs_up"  # ã‚µãƒ ã‚ºã‚¢ãƒƒãƒ—
        elif extended_count == 2:
            if finger_states['index'] and finger_states['middle']:
                return "peace"  # ãƒ”ãƒ¼ã‚¹ã‚µã‚¤ãƒ³
            elif finger_states['thumb'] and finger_states['index']:
                return "gun"  # éŠƒã®å½¢
        elif extended_count == 5:
            return "open_hand"  # é–‹ã„ãŸæ‰‹
        
        return "unknown"  # ä¸æ˜
    
    def _get_empty_result(self) -> Dict:
        """ç©ºã®çµæœ"""
        return {
            'hands_detected': False,
            'hand_count': 0,
            'hands': [],
            'processing_time': 0
        }
    
    def draw_landmarks(self, frame: np.ndarray, detection_result: Dict, 
                      draw_connections: bool = True) -> np.ndarray:
        """ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»"""
        if not detection_result['hands_detected']:
            return frame
        
        try:
            for hand_info in detection_result['hands']:
                landmarks = hand_info['landmarks']
                
                if draw_connections:
                    # æ¥ç¶šç·šæç”»
                    self.mp_drawing.draw_landmarks(
                        frame,
                        landmarks,
                        self.mp_hands.HAND_CONNECTIONS,
                        self.mp_drawing_styles.get_default_hand_landmarks_style(),
                        self.mp_drawing_styles.get_default_hand_connections_style()
                    )
                else:
                    # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®ã¿æç”»
                    self.mp_drawing.draw_landmarks(
                        frame,
                        landmarks,
                        None,
                        self.mp_drawing_styles.get_default_hand_landmarks_style()
                    )
                
                # æ‰‹ã®æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæç”»
                self._draw_hand_info(frame, hand_info)
        
        except Exception as e:
            print(f"âš ï¸  ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æç”»ã‚¨ãƒ©ãƒ¼: {e}")
        
        return frame
    
    def _draw_hand_info(self, frame: np.ndarray, hand_info: Dict):
        """æ‰‹ã®æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæç”»"""
        height, width = frame.shape[:2]
        
        # æ‰‹é¦–ä½ç½®ã‹ã‚‰æƒ…å ±è¡¨ç¤ºä½ç½®è¨ˆç®—
        wrist_x, wrist_y = hand_info['wrist_position']
        text_x = int(wrist_x * width)
        text_y = int(wrist_y * height) - 10
        
        # æ‰‹ã®æƒ…å ±
        label = hand_info['label']
        confidence = hand_info['confidence']
        gesture = hand_info['gesture']
        
        # ãƒ†ã‚­ã‚¹ãƒˆæç”»
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.5
        color = (255, 255, 255)
        thickness = 1
        
        # èƒŒæ™¯çŸ©å½¢
        text = f"{label}: {gesture} ({confidence:.2f})"
        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)
        cv2.rectangle(frame, (text_x - 5, text_y - text_height - 5), 
                     (text_x + text_width + 5, text_y + 5), (0, 0, 0), -1)
        
        # ãƒ†ã‚­ã‚¹ãƒˆ
        cv2.putText(frame, text, (text_x, text_y), font, font_scale, color, thickness)
    
    def get_performance_stats(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
        if not self.detection_times:
            return {}
        
        return {
            'avg_detection_time': sum(self.detection_times) / len(self.detection_times),
            'max_detection_time': max(self.detection_times),
            'min_detection_time': min(self.detection_times),
            'detection_count': self.detection_count,
            'fps': 1.0 / (sum(self.detection_times) / len(self.detection_times)) if self.detection_times else 0
        }
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        if self.hands:
            self.hands.close()
        print("Hand Detector ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å®Œäº†")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
if __name__ == "__main__":
    print("ğŸ” Hand Detector ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    config = {
        'ai_processing': {
            'vision': {
                'hand_detection': {
                    'max_num_hands': 2,
                    'min_detection_confidence': 0.7,
                    'min_tracking_confidence': 0.5,
                    'model_complexity': 1
                }
            }
        }
    }
    
    detector = HandDetector(config)
    
    # ã‚«ãƒ¡ãƒ©ãƒ†ã‚¹ãƒˆ
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“")
        exit()
    
    print("ğŸ‘‹ æ‰‹æ¤œå‡ºãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆESCã§çµ‚äº†ï¼‰...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # æ‰‹æ¤œå‡º
        result = detector.detect_hands(frame)
        
        # çµæœæç”»
        frame_with_landmarks = detector.draw_landmarks(frame, result, draw_connections=True)
        
        # çµæœè¡¨ç¤º
        cv2.imshow('Hand Detection Test', frame_with_landmarks)
        
        if cv2.waitKey(1) & 0xFF == 27:  # ESCã‚­ãƒ¼
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = detector.get_performance_stats()
    print(f"ğŸ“Š æ€§èƒ½çµ±è¨ˆ: {stats}")
    
    detector.cleanup()
    print("âœ… Hand Detector ãƒ†ã‚¹ãƒˆå®Œäº†")
