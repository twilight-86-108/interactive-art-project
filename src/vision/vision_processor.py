import cv2
import time
from typing import Dict, Optional
from .face_detector import FaceDetector
from .hand_detector import HandDetector
from ..emotion.emotion_analyzer import EmotionAnalyzer, Emotion

class VisionProcessor:
    """çµ±åˆç”»åƒå‡¦ç†ãƒ»AI ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDay 3ç‰ˆï¼‰"""
    
    def __init__(self, config: dict):
        self.config = config
        
        # AI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.face_detector = FaceDetector(config)
        self.hand_detector = HandDetector(config)
        self.emotion_analyzer = EmotionAnalyzer(config)
        
        # å‡¦ç†çŠ¶æ…‹
        self.processing_enabled = True
        self.debug_mode = config.get('debug_mode', False)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.total_processing_times = []
        self.frame_count = 0
        
        print("âœ… Vision Processor çµ±åˆåˆæœŸåŒ–å®Œäº†")
    
    def process_frame(self, frame: np.ndarray) -> Dict:
        """çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†"""
        if not self.processing_enabled or frame is None:
            return self._get_empty_result()
        
        start_time = time.time()
        
        try:
            # 1. é¡”æ¤œå‡º
            face_result = self.face_detector.detect_face(frame)
            
            # 2. æ‰‹æ¤œå‡º
            hand_result = self.hand_detector.detect_hands(frame)
            
            # 3. æ„Ÿæƒ…åˆ†æï¼ˆé¡”ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆï¼‰
            emotion = Emotion.NEUTRAL
            emotion_confidence = 0.0
            if face_result['face_detected']:
                emotion, emotion_confidence = self.emotion_analyzer.analyze_emotion(face_result)
            
            # çµ±åˆçµæœä½œæˆ
            integrated_result = {
                'timestamp': time.time(),
                'frame_processed': True,
                
                # é¡”æ¤œå‡ºçµæœ
                'face': face_result,
                
                # æ‰‹æ¤œå‡ºçµæœ
                'hands': hand_result,
                
                # æ„Ÿæƒ…èªè­˜çµæœ
                'emotion': {
                    'emotion': emotion,
                    'confidence': emotion_confidence,
                    'emotion_name': emotion.value
                },
                
                # çµ±åˆå‡¦ç†æ™‚é–“
                'processing_time': 0  # å¾Œã§è¨­å®š
            }
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            total_time = time.time() - start_time
            integrated_result['processing_time'] = total_time
            
            self.total_processing_times.append(total_time)
            if len(self.total_processing_times) > 100:
                self.total_processing_times.pop(0)
            
            self.frame_count += 1
            
            return integrated_result
            
        except Exception as e:
            print(f"âš ï¸  çµ±åˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_empty_result()
    
    def _get_empty_result(self) -> Dict:
        """ç©ºã®çµæœ"""
        return {
            'timestamp': time.time(),
            'frame_processed': False,
            'face': {'face_detected': False},
            'hands': {'hands_detected': False},
            'emotion': {
                'emotion': Emotion.NEUTRAL,
                'confidence': 0.0,
                'emotion_name': 'neutral'
            },
            'processing_time': 0
        }
    
    def draw_all_results(self, frame: np.ndarray, result: Dict, 
                        show_face: bool = True, show_hands: bool = True, 
                        show_emotion: bool = True) -> np.ndarray:
        """å…¨æ¤œå‡ºçµæœæç”»"""
        if not result['frame_processed']:
            return frame
        
        output_frame = frame.copy()
        
        try:
            # é¡”æ¤œå‡ºçµæœæç”»
            if show_face and result['face']['face_detected']:
                output_frame = self.face_detector.draw_landmarks(
                    output_frame, result['face'], draw_all=False
                )
            
            # æ‰‹æ¤œå‡ºçµæœæç”»
            if show_hands and result['hands']['hands_detected']:
                output_frame = self.hand_detector.draw_landmarks(
                    output_frame, result['hands'], draw_connections=True
                )
            
            # æ„Ÿæƒ…æƒ…å ±æç”»
            if show_emotion:
                output_frame = self._draw_emotion_info(output_frame, result['emotion'])
            
            # çµ±åˆæƒ…å ±æç”»ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
            if self.debug_mode:
                output_frame = self._draw_debug_info(output_frame, result)
        
        except Exception as e:
            print(f"âš ï¸  çµæœæç”»ã‚¨ãƒ©ãƒ¼: {e}")
        
        return output_frame
    
    def _draw_emotion_info(self, frame: np.ndarray, emotion_result: Dict) -> np.ndarray:
        """æ„Ÿæƒ…æƒ…å ±æç”»"""
        height, width = frame.shape[:2]
        
        emotion = emotion_result['emotion']
        confidence = emotion_result['confidence']
        emotion_name = emotion_result['emotion_name']
        
        # æ„Ÿæƒ…ã«å¿œã˜ãŸè‰²è¨­å®š
        emotion_colors = {
            'happy': (0, 255, 255),      # é»„è‰²
            'sad': (255, 0, 0),          # é’
            'angry': (0, 0, 255),        # èµ¤
            'surprised': (255, 0, 255),  # ãƒã‚¼ãƒ³ã‚¿
            'neutral': (255, 255, 255)   # ç™½
        }
        
        color = emotion_colors.get(emotion_name, (255, 255, 255))
        
        # æ„Ÿæƒ…æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆ
        text = f"Emotion: {emotion_name.upper()}"
        confidence_text = f"Confidence: {confidence:.2f}"
        
        # èƒŒæ™¯çŸ©å½¢
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1.0
        thickness = 2
        
        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)
        (conf_width, conf_height), _ = cv2.getTextSize(confidence_text, font, 0.8, thickness)
        
        max_width = max(text_width, conf_width)
        total_height = text_height + conf_height + 20
        
        # å³ä¸Šè§’ã«é…ç½®
        rect_x = width - max_width - 20
        rect_y = 10
        
        cv2.rectangle(frame, (rect_x - 10, rect_y), 
                     (rect_x + max_width + 10, rect_y + total_height + 10), 
                     (0, 0, 0), -1)
        
        # ãƒ†ã‚­ã‚¹ãƒˆæç”»
        cv2.putText(frame, text, (rect_x, rect_y + text_height + 5), 
                   font, font_scale, color, thickness)
        cv2.putText(frame, confidence_text, (rect_x, rect_y + text_height + conf_height + 15), 
                   font, 0.8, color, thickness)
        
        return frame
    
    def _draw_debug_info(self, frame: np.ndarray, result: Dict) -> np.ndarray:
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±æç”»"""
        height, width = frame.shape[:2]
        
        debug_info = [
            f"Frame: {self.frame_count}",
            f"Processing: {result['processing_time']:.3f}s",
            f"Face: {'YES' if result['face']['face_detected'] else 'NO'}",
            f"Hands: {result['hands']['hand_count']}",
            f"FPS: {self._get_current_fps():.1f}"
        ]
        
        # å·¦ä¸‹ã«æç”»
        y_start = height - len(debug_info) * 25 - 10
        
        for i, info in enumerate(debug_info):
            y_pos = y_start + i * 25
            cv2.putText(frame, info, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        return frame
    
    def _get_current_fps(self) -> float:
        """ç¾åœ¨ã®FPSå–å¾—"""
        if len(self.total_processing_times) < 10:
            return 0.0
        
        recent_times = self.total_processing_times[-10:]
        avg_time = sum(recent_times) / len(recent_times)
        return 1.0 / avg_time if avg_time > 0 else 0.0
    
    def calibrate_emotion_baseline(self, frame_count: int = 30) -> bool:
        """æ„Ÿæƒ…èªè­˜ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print(f"ğŸ¯ æ„Ÿæƒ…èªè­˜ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆ{frame_count}ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰...")
        
        calibration_results = []
        
        # ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—ã—ã¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—")
            return False
        
        print("ğŸ˜ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãªè¡¨æƒ…ã‚’ä¿ã£ã¦ãã ã•ã„...")
        
        for i in range(frame_count):
            ret, frame = cap.read()
            if not ret:
                continue
            
            # é¡”æ¤œå‡ºã®ã¿å®Ÿè¡Œ
            face_result = self.face_detector.detect_face(frame)
            if face_result['face_detected']:
                calibration_results.append(face_result)
            
            # é€²æ—è¡¨ç¤º
            if i % 10 == 0:
                print(f"ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€²æ—: {i}/{frame_count}")
        
        cap.release()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®š
        success = self.emotion_analyzer.calibrate_baseline(calibration_results)
        
        if success:
            print("âœ… æ„Ÿæƒ…èªè­˜ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
        else:
            print("âŒ æ„Ÿæƒ…èªè­˜ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—")
        
        return success
    
    def get_performance_stats(self) -> Dict:
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ"""
        face_stats = self.face_detector.get_performance_stats()
        hand_stats = self.hand_detector.get_performance_stats()
        emotion_stats = self.emotion_analyzer.get_performance_stats()
        
        return {
            'total_frames_processed': self.frame_count,
            'avg_total_time': sum(self.total_processing_times) / len(self.total_processing_times) if self.total_processing_times else 0,
            'current_fps': self._get_current_fps(),
            'face_detection': face_stats,
            'hand_detection': hand_stats,
            'emotion_analysis': emotion_stats
        }
    
    def enable_processing(self, enabled: bool):
        """å‡¦ç†æœ‰åŠ¹/ç„¡åŠ¹åˆ‡æ›¿"""
        self.processing_enabled = enabled
        print(f"Vision Processing: {'ENABLED' if enabled else 'DISABLED'}")
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        self.face_detector.cleanup()
        self.hand_detector.cleanup()
        print("Vision Processor ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å®Œäº†")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
if __name__ == "__main__":
    print("ğŸ” Vision Processor çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    config = {
        'ai_processing': {
            'vision': {
                'face_detection': {
                    'max_num_faces': 1,
                    'refine_landmarks': True,
                    'min_detection_confidence': 0.7
                },
                'hand_detection': {
                    'max_num_hands': 2,
                    'min_detection_confidence': 0.7
                }
            },
            'emotion': {
                'smoothing_window': 10,
                'confidence_threshold': 0.6
            }
        },
        'debug_mode': True
    }
    
    processor = VisionProcessor(config)
    
    # ã‚«ãƒ¡ãƒ©ãƒ†ã‚¹ãƒˆ
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ã‚«ãƒ¡ãƒ©ã‚’é–‹ã‘ã¾ã›ã‚“")
        exit()
    
    print("ğŸ¬ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆC: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³, ESC: çµ‚äº†ï¼‰...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # çµ±åˆå‡¦ç†
        result = processor.process_frame(frame)
        
        # çµæœæç”»
        output_frame = processor.draw_all_results(
            frame, result, 
            show_face=True, show_hands=True, show_emotion=True
        )
        
        # è¡¨ç¤º
        cv2.imshow('Vision Processor Test', output_frame)
        
        key = cv2.waitKey(1) & 0xFF
        if key == 27:  # ESC
            break
        elif key == ord('c'):  # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            processor.calibrate_emotion_baseline()
    
    cap.release()
    cv2.destroyAllWindows()
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = processor.get_performance_stats()
    print(f"ğŸ“Š ç·åˆæ€§èƒ½çµ±è¨ˆ:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    processor.cleanup()
    print("âœ… Vision Processor çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
